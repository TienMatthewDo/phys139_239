\newif\ifshowsolutions
\showsolutionsfalse
\input{../common/preamble}

\chead{
  {\vbox{
      \vspace{2mm}
      \large
      Machine Learning in Physics \hfill
      UCSD PHYS 139/239 \hfill \\[1pt]
      Homework 3\hfill
      Due: Friday, May 10, 2024, 8:00pm\\
	  \hfill
	  Corrections due: Wednesday, May 15, 2024, 8:00pm\\
    }
  }
}

\begin{document}
\pagestyle{fancy}

\section*{Policies}
\begin{itemize}
	\item You are free to collaborate on all of the problems, subject to the collaboration policy stated in the syllabus.
	\item Please submit your report as a single .pdf file to Gradescope under ``Homework 3" or ``Homework 3 Corrections".
	      \textbf{In the report, include any images generated by your code along with your answers to the questions.}
	      For instructions specifically pertaining to the Gradescope submission process, see \url{https://www.gradescope.com/get_started#student-submission}.
	\item Please submit your code as a .zip archive to Gradescope under ``Homework 3 Code'' or ``Homework 3 Code Corrections".
	      The .zip file should contain your code files.
	      Submit your code either as Jupyter notebook .ipynb files or .py files.
\end{itemize}

\newpage
\section{GitHub [20 Points]}
% \materials{lecture 2}

\begin{problem}[20]
Follow the instructions below.
The purpose of this problem is to get you accustomed to Git commands and using GitHub.

\begin{itemize}
	\item Create a GitHub account if you don't have one already: \url{https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-account}.
	\item Create an SSH key (either on your laptop or on DataHub, depending on where you will work with GitHub): \url{https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent}.
	\item Add the SSH key to your GitHub account: \url{https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account}.
	\item Create a \emph{public} GitHub repository called \texttt{<your_github_username>/hello-world}. See \url{https://docs.github.com/en/get-started/quickstart/hello-world}.
	\item Clone the GitHub repository locally on the command line:
	      \begin{lstlisting}[language=bash]
git clone git@github.com:<your_github_username>/hello-world
		\end{lstlisting}
	\item Create and check out a new ``feature branch'' called \texttt{readme_edits}
	      \begin{lstlisting}[language=bash]
cd hello-world
git checkout -b readme_edits
  \end{lstlisting}
	\item Make edits to the \texttt{README.md}, e.g. add a sentence like ``Physics and machine learning are fun!''
	\item Stage and commit your changes with a helpful commit message
	      \begin{lstlisting}[language=bash]
git add README.md
git commit -m "README update"
\end{lstlisting}
	\item Push your local changes to the remote repository
	      \begin{lstlisting}[language=bash]
git push origin readme_edits
\end{lstlisting}
	\item Create a pull request by navigating to the webpage: \url{https://github.com/<your_github_username>/hello-world/pull/new/readme_edits} where you insert your GitHub username.
	      The exact URL should be displayed on the command line.
	\item Check that the changes are what you expect them to be on the \url{https://github.com/<your_github_username>/hello-world/pull/1/files} tab and merge the pull request!
	\item Please provide your GitHub repository URL so that we can check you followed all the steps.
\end{itemize}
\end{problem}

\section{RNNs vs. CNNs for Time Series [40 points]}
% \materials{lectures 9 and 10}

This problem uses the hands-on notebook \url{https://github.com/jmduarte/phys139_239/blob/main/notebooks/05_Time_Series_Data_RNN.ipynb}.
For this problem, we will use the full 50k traces, by setting
\begin{lstlisting}[language=python]
n_train = 40000
n_test = 10000
\end{lstlisting}
We \emph{highly recommend} using the GPU-enabled DataHub for this problem.
Note the training can be sped up by increasing the batch size to, e.g., 2048, but this requires tuning the learning rate and schedule to achieve the same performance.

\begin{problem}[15]
Replace the LSTM layers with bidirectional LSTM layers using the \texttt{layers.Bidirectional} wrapper function: \url{https://keras.io/api/layers/recurrent_layers/bidirectional/}.
Note for the first layer, the \texttt{input_shape} argument needs to be an input to the outer \texttt{layers.Bidirectional} wrapper function instead of the inner \texttt{layers.LSTM} function.
Set \texttt{verbose=1} in the \texttt{model.fit()} command to be able to see the output during training, \texttt{batch\_size=2048} to speed up the training, and \texttt{epochs=100} to train the model for (up to) 100 epochs.
How many trainable parameters does the model have?
How long does 1 epoch of training take (approximately)?
What accuracy and AUC do you achieve for the classification task?
\end{problem}


\begin{problem}[15]
Now replace the LSTM layers with 1D convolutional layers with the hyperparameters indicated in the notebook.
How many trainable parameters does the model have?
How long does 1 epoch of training take (approximately)?
What accuracy and AUC do you achieve for the classification task?
\end{problem}

\begin{problem}[10]
From an accuracy/AUC perspective, how do the two models compare?
Which model trains faster?
\end{problem}

\end{document}
